{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN MODEL FOR PART A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mobby/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#loading packages\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm \n",
    "import codecs\n",
    "import random\n",
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. VISUALIZING DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('offenseval-training-v1.tsv',sep='\\t', header=0, names = ['ID','Tweet','Task_A','Task_B','Task_C'])\n",
    "df_test = pd.read_csv('offenseval-trial.txt',sep=\"\\t\", header=0, names = ['Tweet','Task_A','Task_B','Task_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Task_A</th>\n",
       "      <th>Task_B</th>\n",
       "      <th>Task_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@LeftyGlenn @jaredeker @BookUniverse @hashtagz...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hot Mom Sucks Off Step Son In Shower 8 min htt...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bro these are some cute butt plugs I‚Äôm trying ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona Supreme Court strikes down state legis...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arguing gun control is wrong of me whoever has...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet Task_A Task_B Task_C\n",
       "0  @LeftyGlenn @jaredeker @BookUniverse @hashtagz...    NOT    NaN    NaN\n",
       "1  Hot Mom Sucks Off Step Son In Shower 8 min htt...    OFF    UNT    NaN\n",
       "2  bro these are some cute butt plugs I‚Äôm trying ...    OFF    UNT    NaN\n",
       "3  Arizona Supreme Court strikes down state legis...    NOT    NaN    NaN\n",
       "4  Arguing gun control is wrong of me whoever has...    NOT    NaN    NaN"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process functions \n",
    "\n",
    "def remove_emoji(sentence):\n",
    "    \n",
    "    #processed = sentence.decode('utf-8')\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    processed = emoji_pattern.sub(r'', sentence) # no emoji\n",
    "    \n",
    "    return(processed)\n",
    "\n",
    "def replace_users(sentence):\n",
    "    \n",
    "    processed = re.sub('(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)','@USER', sentence)\n",
    "    return(processed)\n",
    "\n",
    "def replace_url(sentence):\n",
    "    \n",
    "    processed = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','website', sentence)\n",
    "    return(processed)\n",
    "    \n",
    "\n",
    "def remove_white(sentence):\n",
    "    \n",
    "    processed = sentence.replace(r'[^\\w\\d\\s]',' ') #replace punctuation by space\n",
    "    processed = processed.replace(r'\\s+', ' ') #replace whitespaces with a single space\n",
    "    processed = processed.replace(r'^\\s+|\\s+?$','') #replace leading and trailing whitespace\n",
    "    processed = processed.replace('\\n','')\n",
    "\n",
    "    return(processed)\n",
    "\n",
    "\n",
    "def remove_stop(sentence): #remove stop words\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed = sentence.apply(lambda x: ' '.join(term for term in x.split() is term not in stop_words))\n",
    "    \n",
    "    return(processed)\n",
    "\n",
    "def remove_stems(sentence):\n",
    "    \n",
    "    ps = nltk.PorterStemmer()\n",
    "    \n",
    "    processed = processed.apply(lambda x: ' '.join(ps.stem(term) for term in x.split()))\n",
    "    \n",
    "    return(processed)\n",
    "\n",
    "def preproc_dset(corpus):\n",
    "    \n",
    "    new_corpus = []\n",
    "    \n",
    "    for sentence in corpus :\n",
    "        \n",
    "        processed = remove_white(sentence)\n",
    "        processed = remove_stop(processed)\n",
    "        processed = remove_stems(processed)\n",
    "        new_corpus.append(processed)\n",
    "        \n",
    "    return(new_corpus)\n",
    "\n",
    "\n",
    "def remove_char(sentence):\n",
    "    \n",
    "    chars = ['.',',',':','!','?','%',')','(',';','[',']','{','}','$','@','#','=','^','*','$','/']\n",
    "    for char in chars:\n",
    "        sentence= sentence.replace(char,'')\n",
    "        \n",
    "        \n",
    "    return(sentence)\n",
    "\n",
    "def lower(sentence):\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    return(sentence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_corpus(corpus):\n",
    "    \n",
    "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        \n",
    "        tokenized_sentence = []\n",
    "        for token in sentence.split(' '): # simplest split is \n",
    "            tokenized_sentence.append(token)\n",
    "           \n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "        \n",
    "    return tokenized_corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size : 13240\n",
      "labels size : 13240\n",
      "corpus size : 319\n",
      "labels size : 319\n"
     ]
    }
   ],
   "source": [
    "corpus = df['Tweet'].tolist()\n",
    "labels = df['Task_A'].tolist()\n",
    "\n",
    "#corpus = df['Tweet']\n",
    "#labels = df['Task_A']\n",
    "\n",
    "test_corpus = df_test['Tweet'].tolist()\n",
    "test_labels = df_test['Task_A'].tolist()\n",
    "\n",
    "print(\"corpus size :\", len(corpus))\n",
    "print(\"labels size :\", len(labels))\n",
    "\n",
    "print(\"corpus size :\", len(test_corpus))\n",
    "print(\"labels size :\", len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 1 1 1 0 1]\n",
      "['OFF', 'OFF', 'NOT', 'OFF', 'NOT', 'OFF', 'OFF', 'OFF', 'NOT', 'OFF']\n"
     ]
    }
   ],
   "source": [
    "#convert class labels to binary values \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder_test = LabelEncoder()\n",
    "\n",
    "Y = encoder.fit_transform(labels)\n",
    "Y_test = encoder_test.fit_transform(test_labels)\n",
    "\n",
    "print(Y[:10])\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of sentence before removing stopwords : 14\n",
      "size of sentence after removing stopwords 5\n"
     ]
    }
   ],
   "source": [
    "##### PROCESS TRAINING DATA \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed_corpus = []\n",
    "\n",
    "for sentence in corpus :\n",
    "    processed = remove_emoji(sentence)\n",
    "    processed = remove_white(processed) #remove punctuation + white space\n",
    "    processed = remove_char(processed)\n",
    "    processed = lower(processed)\n",
    "    processed_corpus.append(processed)\n",
    "\n",
    "tokenized_corpus = get_tokenized_corpus(processed_corpus) #tokenize corpus\n",
    "\n",
    "print('size of sentence before removing stopwords :',len(tokenized_corpus[0]))\n",
    "\n",
    "filtered_corpus = []\n",
    "for sentence in tokenized_corpus :\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        if word not in stop_words and word!='': #remove stop words and empty words\n",
    "            new_sentence.append(word)       \n",
    "    filtered_corpus.append(new_sentence)\n",
    "    \n",
    "    \n",
    "print('size of sentence after removing stopwords',len(filtered_corpus[0]))\n",
    "\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "stemmed_corpus = []\n",
    "for sentence in filtered_corpus :\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        new_sentence.append(ps.stem(word)) #stem words      \n",
    "    stemmed_corpus.append(new_sentence)\n",
    "\n",
    "#print('corpus insight after pre-processing :',stemmed_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of sentence before removing stopwords : 41\n",
      "size of corpus : 13240\n",
      "size of sentence after removing stopwords 23\n"
     ]
    }
   ],
   "source": [
    "######## PROCESS TEST DATA\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "test_processed_corpus = []\n",
    "\n",
    "for sentence in test_corpus :\n",
    "    processed = remove_emoji(sentence)\n",
    "    processed = replace_users(processed)\n",
    "    processed = replace_url(processed)\n",
    "    processed = remove_white(processed) #remove punctuation + white space\n",
    "    processed = remove_char(processed)\n",
    "    processed = lower(processed)\n",
    "    test_processed_corpus.append(processed)\n",
    "\n",
    "test_tokenized_corpus = get_tokenized_corpus(test_processed_corpus) #tokenize corpus\n",
    "\n",
    "print('size of sentence before removing stopwords :',len(test_tokenized_corpus[0]))\n",
    "\n",
    "test_filtered_corpus = []\n",
    "for sentence in test_tokenized_corpus :\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        if word not in stop_words and word!='': #remove stop words and empty words\n",
    "            new_sentence.append(word)       \n",
    "    test_filtered_corpus.append(new_sentence)\n",
    "    \n",
    "print('size of corpus :', len(filtered_corpus))\n",
    "    \n",
    "print('size of sentence after removing stopwords',len(test_filtered_corpus[0]))\n",
    "\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "test_stemmed_corpus = []\n",
    "for sentence in test_filtered_corpus :\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        new_sentence.append(ps.stem(word)) #stem words      \n",
    "    test_stemmed_corpus.append(new_sentence)\n",
    "\n",
    "#print('\\ncorpus insight after pre-processing :',test_stemmed_corpus[:20])\n",
    "#print('\\ncorpus before any pre-processing :', test_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words : 18370\n",
      "Most common words : [('user', 33386), ('url', 2054), ('liber', 1576), ('gun', 1503), ('like', 1186), ('control', 1183), ('antifa', 1112), ('conserv', 1006), ('maga', 992), ('get', 902)]\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "\n",
    "for sentence in stemmed_corpus :\n",
    "    for word in sentence :\n",
    "        all_words.append(word)\n",
    "        \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "print('Number of words :',len(all_words))\n",
    "print('Most common words :', all_words.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:1500] #10k words as features -------> HYPER-PARAMETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['user', 'northern', 'az', 'good', 'look', 'forward', 'see', 'maga'], 0)\n",
      "[(['user', \"he'\", 'use', 'administr', 'sell', 'uranium', 'profit', 'spi', 'polit', 'oppon', 'bow', 'enemi', 'probabl', 'best', 'draintheswamp'], 0), (['oncologist', 'discuss', 'use', 'medic', 'marijuana', 'patient', 'new', 'studi', 'find', 'websit', 'websit'], 0), (['user', 'gemini‚Äô', 'fav', 'fuck', 'hater', 'tbh'], 1), (['guess', 'ima', 'beat', 'lil', 'kid', 'ass', 'today', 'caus', 'lil', 'becki', 'kid', 'get', 'away', 'websit'], 1), (['user', 'user', 'user', 'user', 'user', 'user', 'found', 'insect', 'egg', '‚Äúmedic', 'marijuana‚Äù', 'curaleaf', 'cost', '55', '18', 'socialjustic', 'websit'], 0), (['user', 'user', 'user', 'user', 'user', 'user', 'fine', 'could', 'afford', 'gun', 'want', 'could', 'fit', 'budget', 'budget', 'fine', 'canada', 'gun', 'insur', 'gun', 'control', 'lotsa', 'p'], 0), (['im', 'think', 'read', 'cathedral\"', 'back', 'ap', 'british', 'lit', '\"the', 'a&ampp\"', 'first', 'year', 'colleg', 'fuckin', 'bore', '\"'], 1), (['arizona', 'suprem', 'court', 'schedul', 'rule', 'legal', 'medic', 'marijuana', 'colleg', 'campus', 'websit'], 0), (['sasha', 'realli', 'came', 'jugular', 'call', 'scorpio', 'fetish', 'last', 'night', 'bxtch'], 1), (['microdos', 'futur', 'cannabi', '|', 'medic', 'marijuana', 'news-', 'mmj', 'websit'], 0)]\n"
     ]
    }
   ],
   "source": [
    "dset = zip(stemmed_corpus,Y)\n",
    "d = list(dset)\n",
    "\n",
    "test_dset = zip(test_stemmed_corpus,Y_test)\n",
    "test_d = list(test_dset)\n",
    "\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(d)\n",
    "\n",
    "np.random.shuffle(test_d)\n",
    "\n",
    "print(d[0])\n",
    "print(test_d[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(sentence):\n",
    "    \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in sentence)\n",
    "        \n",
    "    return (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', 'ask', 'nativ', 'american', 'take']\n",
      "{'user': True, 'ask': True, 'nativ': True, 'american': True, 'take': True, 'go': False, 'home': False, 'you‚Äôr': False, 'drunk': False, 'maga': False, 'trump2020': False, 'url': False, 'amazon': False, 'investig': False, 'chines': False, 'employe': False, 'sell': False, 'intern': False, 'data': False, 'third-parti': False, 'seller': False, 'look': False, 'edg': False, 'competit': False, 'marketplac': False, 'kag': False, 'china': False, 'tcot': False, 'someon': False, 'should\\'vetaken\"': False, 'piec': False, 'shit': False, 'volcano': False, '\"': False, 'obama': False, 'want': False, 'liber': False, '&amp': False, 'illeg': False, 'move': False, 'red': False, 'state': False, 'kookoo': False, 'oh': False, 'noe': False, 'tough': False, 'liter': False, 'talk': False, 'lol': False, 'mass': False, 'shoot': False, 'like': False, 'set': False, 'up': False, 'it‚Äô': False, 'propaganda': False, 'use': False, 'divid': False, 'us': False, 'major': False, 'issu': False, 'gun': False, 'control': False, 'terror': False, 'buy': False, 'icecream': False, 'canada': False, 'doesn‚Äôt': False, 'need': False, 'anoth': False, 'cuck': False, 'alreadi': False, 'enough': False, 'looneyleft': False, 'fking': False, 'great': False, 'countri': False, 'qproof': False, 'trudeaumustgo': False, 'fault': False, 'support': False, 'what‚Äô': False, 'differ': False, 'kavanaugh': False, 'one': False, 'men': False, 'admit': False, 'grope': False, '15': False, 'year': False, 'old': False, 'girl': False, 'ago': False, 'confirm': False, 'scj': False, 'demsarefraud': False, 'demsaredon': False, 'walkawaydemocrats2018': False, 'redwav': False, 'voteredsaveamerica': False, 'trumptrain': False, 'lie': False, 'corrupt': False, 'traitor': False, 'nobodi': False, 'hear': False, 'anymor': False, 'deepstatecorrupt': False, 'scare': False, 'everi': False, 'play': False, 'hockey': False, 'warp': False, 'puck': False, 'soda': False, 'boarder': False, 'lot': False, 'ice': False, 'also': False, 'king': False, 'tast': False, 'sing': False, 'listen': False, '‚ô•Ô∏è': False, 'love': False, 'never': False, 'hurt': False, '‚úîÔ∏è': False, 'vote': False, 'gop': False, 'watch': False, \"liberals'\": False, 'dirt': False, '55': False, 'time': False, 'right': False, 'hous': False, 'respond': False, 'concern': False, 'canadian': False, 'four': False, 'five': False, 'stronger': False, 'good': False, 'reason‚Äù': False, 'guncontrol': False, 'cdnpoli': False, 'cdnhist\"': False, 'besid': False, 'jax‚Äô': False, 'mom': False, 'mayb': False, 'ope': False, 'hand': False, 'favorit': False, 'he‚Äô': False, 'person': False, 'show': False, 'kid': False, 'throw': False, 'bullshit': False, 'flag': False, 'nonsens': False, 'putuporshutup': False, 'callthevotealreadi': False, 'correct': False, 'kind': False, 'conserv': False, 'wanna': False, 'associ': False, 'everyon': False, 'left': False, 'communist': False, 'antifa': False, 'member': False, 'da': False, 'fuck': False, 'peopl': False, \"there'\": False, \"men'\": False, 'room': False, \"women'\": False, 'pick': False, 'stick': False, 'w': False, 'ü§î': False, 'willi': False, 'fan': False, 'sinc': False, 'bornlov': False, 'hold': False, 'ralli': False, 'beto': False, 'exactli': False, 'furiou': False, 'could': False, 'give': False, 'specif': False, 'tbh': False, 'day': False, 'gener': False, 'connect': False, 'annoy': False, 'south': False, 'korean': False, 'offici': False, '‚Äúleader': False, 'discuss': False, 'denuk': False, 'measur': False, 'pyongyang‚Äù\"': False, 'rednationrising\"': False, 'tell': False, 'hooper': False, 'feel': False, 'better': False, 'chase': False, 'titl': False, 'refer': False, 'hillari': False, 'know': False, 'tiresom': False, 'berni': False, 'bash': False, 'clinton': False, 'actual': False, 'democrat': False, 'rais': False, 'money': False, 'parti': False, 'posit': False, 'messag': False, 'glad': False, 'see': False, 'friend': False, 'meti': False, 'much': False, 'zionist': False, 'ye': False, 'movi': False, 'ü§∑\\u200d‚ôÇÔ∏è': False, 'thing': False, 'stall': False, 'stop': False, 'trump': False, 'presid': False, 'done': False, 'late': False, 'demsuck': False, 'wonder': False, 'apologet': False, 'social': False, 'updat': False, 'end': False, 'blatant': False, 'racism': False, 'windrush': False, 'grenfel': False, 'prove': False, 'alloutpolit': False, 'politicsl': False, 'pmq': False, 'happen': False, 'i‚Äôm': False, 'lay': False, 'bed': False, 'cri': False, 'kelli': False, 'clarkson': False, 'thank': False, 'free': False, 'speech': False, 'smart': False, 'think': False, 'gen': False, 'flynn‚Äô': False, 'sentenc': False, 'keep': False, 'reschedul': False, \"that'\": False, 'expect': False, 'placat': False, 'violent': False, 'leftiststerrorist': False, 'kavanaughconfirm': False, 'woke': False, 'blow': False, 'hard': False, 'mean': False, 'max': False, 'lvl': False, 'twitter': False, 'tri': False, 'new': False, 'game': False, 'pleas': False, 'explain': False, 'opposition\"': False, 'bernier': False, 'escap': False, 'split': False, 'rest': False, \"he'\": False, 'even': False, 'racist': False, 'exclusionari': False, 'be\"': False, 'socialist': False, 'polit': False, 'stand': False, 'downtrodden': False, 'well': False, 'conflat': False, 'critic': False, 'israel': False, 'anti': False, 'semit': False, 'part': False, 'problem': False, '3': False, 'nina': False, 'reason': False, 'read': False, 'book': False, 'long': False, 'worri': False, 'charact': False, 'trust': False, 'goddamn': False, 'goddess': False, 'would': False, 'die': False, 'thousand': False, 'big': False, 'bold': False, 'perfect': False, 'arc': False, 'fit': False, 'scotu': False, 'alleg': False, 'women': False, 'shadi': False, 'financi': False, 'disqualifi': False, 'ruleoflaw': False, 'matter': False, 'metoo': False, 'countryoverparti': False, 'withdrawkavanaugh': False, 'stopkavanaugh': False, 'ppl': False, 'say': False, \"i'm\": False, 'racist\"': False, 'repeat': False, 'me\"': False, 'kathi': False, 'patriot': False, 'spine': False, 'steel': False, 'chang': False, 'trade': False, 'everyth': False, 'americafirst': False, 'usa': False, 'leadership': False, 'respect': False, 'economi': False, 'job': False, 'peacethrustrength': False, 'god': False, 'bless': False, 'theusa': False, 'wow': False, 'realli': False, 'don‚Äôt': False, 'sens': False, 'humor': False, 'readi': False, 'holi': False, 'moli': False, 'cathol': False, 'awaken': False, 'falseprophet': False, 'lunat': False, 'yeah': False, 'per': False, 'dina': False, 'everyday': False, 'claudia': False, 'hors': False, 'got': False, 'vet': False, 'care': False, 'unless': False, 'donat': False, 'exampl': False, 'limp': False, 'etc': False, 'surpris': False, 'blm': False, 'soro': False, 'america': False, 'worst': False, 'enemi': False, 'histori': False, 'believ': False, 'way': False, 'with': False, 'he': False, 'gunna': False, 'wed': False, 'best': False, 'pretti': False, 'conflict': False, 'here‚Äîdon‚Äôt': False, 'involv': False, 'agenda': False, 'announc': False, 'come': False, 'fruition': False, 'basic': False, 'steal': False, \"labour'\": False, 'idea': False, 'get': False, '4': False, '10': False, 'british': False, 'full-on': False, 'voter': False, 'coincid': False, 'advoc': False, 'must': False, 'fall': False, 'assur': False, 'elector': False, 'horrif': False, '2a': False, 'hi': False, 'huge': False, 'camila': False, 'mend': False, 'alright': False, 'tweet': False, 'promot': False, 'materi': False, 'certainli': False, 'weird': False, 'view': False, 'toward': False, 'margaret': False, 'sanger': False, 'geonicid': False, 'philosophi': False, 'abort': False, 'minor': False, 'i‚Äôd': False, 'black': False, 'popul': False, 'suffer': False, 'share': False, 'opinion': False, 'prison': False, 'system': False, 'still': False, 'away': False, 'potenti': False, 'murder': False, 'intent': False, 'hurrican': False, 'louisiana': False, '500': False, 'inmat': False, 'went': False, 'missing\"\"': False, 'useless': False, 'attempt': False, 'follow': False, 'walkaway': False, 'liberalsyour': False, 'poison': False, 'grate': False, 'dog': False, 'white': False, 'cruel': False, 'man': False, 'fact': False, 'pay': False, 'kick': False, 'dem': False, 'butt': False, '-': False, 'fun': False, 'dumb': False, 'dumber': False, 'two': False, 'toxictori': False, 'minist': False, 'expens': False, '¬£50': False, 'chariti': False, 'toriesmustgo': False, 'dissolvetheunion': False, 'ur': False, 'straight': False, 'forword': False, 'saw': False, 'u': False, 'danc': False, 'dewwan': False, 'ky': False, 'ap': False, 'kitnay': False, 'porrani': False, 'ho': False, 'industri': False, 'mein': False, 'qualiti': False, 'gather': False, 'senior': False, 'artist': False, 'manmarziyaan': False, 'ü§†': False, 'clown': False, 'silenc': False, 'alex': False, 'jone': False, 'retali': False, 'jack': False, 'question': False, 'that': False, 'obstructionist': False, 'anti-american': False, 'pharise': False, 'run': False, 'noth': False, \"notyou'r\": False, '2': False, 'repres': False, 'alinskyit': False, 'orwellian': False, 'leftist': False, 'tool&ampfool': False, 'thingslik': False, '999': False, 'marxists&ampantifa': False, 'similar': False, '2upsst': False, 'b4u': False, 'word': False, 'fascist': False, 'fascism': False, 'ahead': False, \"d'souza'\": False, 'lie\"right': False, 'squat': False, 'either\"': False, 'rememb': False, 'somehow': False, 'centrist\"': False, 'antifa-hat': False, 'quit': False, 'seem': False, 'bring': False, 'condemn': False, 'things\"': False, 'work': False, 'last': False, 'wh': False, \"trump'\": False, 'mind': False, 'april': False, 'ryan': False, 'flatter': False, 'import': False, 'principl': False, 'hypocricket': False, 'forget': False, 'back': False, 'beauti': False, 'teach': False, 'valu': False, 'royalti': False, 'head': False, 'stay': False, 'real': False, '‚ù§Ô∏è': False, 'crimin': False, 'son': False, 'campaign': False, 'didn‚Äôt': False, 'septemb': False, '11th': False, 'stori': False, 'came': False, 'togeth': False, 'stood': False, 'shoulder': False, 'made': False, 'proud': False, 'yet': False, 'damn': False, 'cannot': False, 'moment‚Äîprotest': False, 'saturday': False, 'tear': False, '1': False, 'water': False, '99': False, 'address': False, 'nra': False, 'let': False, 'down\"': False, 'drink': False, 'koolaid': False, 'she‚Äô': False, 'bloat': False, 'holder': False, 'prosecut': False, 'player': False, 'color': False, 'institut': False, 'behav': False, 'window': False, 'licker': False, 'babysit': False, 'people‚Äùhow': False, 'is‚Äù': False, '‚Äúshe': False, 'three': False, 'kids‚Äù': False, 'medont': False, 'mad': False, 'wtc': False, 'threat': False, 'qanon': False, 'wakeupamerica': False, 'easili': False, 'digest': False, 'content': False, 'nuanc': False, 'understand': False, 'pictur': False, 'antifa\"': False, 'base': False, 'level': False, 'emot': False, 'respons': False, 'requir': False, 'order': False, 'profitable\"': False, 'plain': False, 'can‚Äôt': False, 'convers': False, 'news': False, 'moonbeam': False, 'coma': False, 'omg': False, 'extra': False, 'ili': False, 'call': False, 'isa': False, 'narcissist': False, 'sociopath': False, 'unfortun': False, 'exam': False, 'group': False, 'alivebut': False, 'career': False, 'railroad': False, 'odd': False, 'herespeci': False, 'guy': False, 'alway': False, 'presentbet': False, 'babi': False, 'persuas': False, 'fam': False, 'anywayw': False, 'novemb': False, 'burt': False, 'cool': False, 'dude': False, 'final': False, 'point': False, 'okay': False, 'heat': False, 'happi': False, 'huh': False, 'former': False, 'arrest': False, 'citizen': False, 'em': False, 'moveh': False, 'loser': False, \"let'\": False, 'bottom': False, 'dm': False, 'zipcod': False, \"i'll\": False, 'check': False, 'make': False, 'sure': False, 'enjoy': False, 'tv': False, 'offer': False, 'asap': False, 'nevinbruc': False, 'record': False, 'doug': False, 'sen': False, 'bc': False, 'credibl': False, 'choic': False, 'won‚Äôt': False, 'next': False, 'elect': False, 'president‚Äô': False, 'total': False, 'opposit': False, 'cnn‚Äô': False, 'cnn': False, 'they‚Äôr': False, 'narr': False, 'par': False, 'nazi': False, 'punch': False, 'side': False, 'coin\"': False, 'noooooooo': False, 'extermin': False, 'deport': False, 'poc': False, 'jew': False, 'prevent': False, 'comparison': False, 'benefit': False, 'fash\"': False, 'mani': False, 'argument': False, 'may': False, 'dumbest': False, 'common': False, 'idk': False, \"what'\": False, 'yall': False, 'chill': False, 'abysm': False, '2nd': False, 'half': False, 'rizzo': False, '1st': False, 'basebal': False, 'power': False, 'number': False, 'drop': False, 'bounc': False, 'anytim': False, 'lmao': False, 'ps': False, 'pack': False, \"can't\": False, 'wear': False, 'hat': False, 'without': False, 'abus': False, 'meet': False, 'anim': False, 'attack': False, 'branch': False, 'communist-socialist-nazi': False, 'laugh': False, 'i‚Äôv': False, 'gotten': False, 'first': False, 'all‚Äî': False, 'pizza': False, 'panera': False, 'kill': False, 'shanahan': False, 'spoon': False, '100': False, 'healthi': False, 'obvious': False, 'excus': False, '‚Äúall': False, 'doom‚Äù': False, 'sort': False, 'ugli': False, 'heart': False, 'troll': False, 'clearli': False, 'pokemon': False, 'master': False, 'almost': False, '400': False, 'rare': False, 'candi': False, 'alon': False, '150': False, 'legaci': False, 'moveset': False, 'depend': False, 'collect': False, 'dont': False, 'hype': False, 'protest': False, 'report': False, 'sunderland': False, 'except': False, 'footbal': False, 'westbrom': False, 'wors': False, 'type': False, 'mega': False, 'mbga': False, 'mcga': False, 'muslim': False, 'farleft': False, 'doom': False, 'lose': False, 'realis': False, 'civilwar': False, 'soon': False, \"they'll\": False, 'claim': False, 'comment': False, 'taken': False, 'context': False, 'integrity\"\"': False, 'save': False, 'honor': False, 'decenc': False, 'yoyo': False, 'rich': False, 'riddanc': False, 'xd': False, 'case': False, 'cuas': False, 'laught': False, 'dead': False, 'poor': False, 'thing\"': False, 'standin': False, '\"nope': False, 'suffering\"': False, '\"so': False, 'dead\"': False, '\"yeah': False, 'dead\"\"': False, 'probabl': False, 'imagin': False, 'sjw': False, 'snowflak': False, 'safe\"': False, 'might': False, 'lurk': False, 'shadow': False, 'ruski': False, '21st': False, 'century\"': False, 'light': False, 'tori': False, 'partyveri': False, 'hater': False, 'delet': False, 'ig': False, 'blame': False, 'trudeau': False, 'cost': False, 'standard': False, 'push': False, 'warn': False, 'josh': False, 'gordon': False, 'month': False, 'daili': False, 'basi': False, 'shock': False, 'transpir': False, 'today': False, 'sjshsj': False, 'bitch': False, 'haiti': False, 'collus': False, 'dumocrat': False, 'skill': False, 'amaz': False, 'fist': False, 'pump': False, 'troop': False, 'farakan': False, 'detest': False, 'simpli': False, 'result': False, 'bad': False, 'supporters\"': False, 'riot': False, 'street': False, 'counterpart': False, 'thought': False, 'strict': False, 'help': False, 'sicken': False, 'bias': False, 'twitter‚Äô': False, 'manag': False, 'twitterleftbia': False, 'life': False, 'tho': False, 'midterm': False, 'fals': False, 'spread': False, 'naacp': False, 'death': False, 'professor': False, 'ford': False, 'threaten': False, 'violenc': False, 'candid': False, 'spanki': False, 'impeach': False, 'seen': False, 'pal': False, 'littl': False, 'bat': False, 'someth': False, 'you-': False, 'bit': False, '&gterad': False, 'hiv': False, 'malaria': False, 'middl': False, 'africa': False, 'bloodi': False, 'luck': False, 'interact': False, 'volunt': False, 'fcp': False, 'wit': False, 'demand': False, 'float': False, 'parad': False, 'ableg': False, 'ad': False, 'video': False, 'playlist': False, 'combat': False, 'mon': False, 'frere': False, 'vs': False, 'ismael': False, 'et': False, 'assassin': False, 'david': False, 'le': False, 'boxer': False, 'yfc': False, 'law': False, 'changedy': False, 'guilti': False, 'proven': False, 'innoc': False, 'mental': False, 'ill': False, 'joe': False, 'biden': False, 'outcom': False, 'name': False, 'hill': False, 'suck': False, 'donkey': False, 'ball': False, 'yo': False, 'gorgeou': False, 'absolut': False, 'nail': False, 'coffin': False, 'hilliari': False, 'hope': False, '‚ù§': False, '‚Äúthey‚Äù': False, 'failur': False, 'refus': False, 'realiti': False, 'overwhelm': False, 'polici': False, 'clue': False, 'california': False, 'biggest': False, 'partli': False, 'fund': False, 'dumbass': False, 'levistrauss': False, 'heck': False, 'freedom': False, '~': False, 'select': False, 'pre': False, 'sht': False, 'neither': False, 'ok': False, 'poll': False, 'donni': False, 'jr': False, 'famili': False, 'owe': False, 'apolog': False, 'activ': False, 'line': False, 'pocket': False, 'father': False, 'unfit': False, 'mockeri': False, 'republ': False, 'shame': False, 'justic': False, 'prevail': False, 'harvardharri': False, 'conduct': False, 'found': False, '48': False, 'percent': False, 'said': False, '250000': False, 'legal': False, 'immigr': False, 'brought': False, 'year\"': False, 'tcot\"': False, 'doubt': False, 'becom': False, 'nation': False, 'worker': False, 'gone': False, 'act': False, 'der': False, 'sturmer': False, 'einsatzgruppenwaffen': False, 'ss': False, 'detach': False, 'carmen': False, 'frick': False, 'cute': False, 'chelsea': False, 'crazi': False, 'mother': False, 'binnedbori': False, 'sent': False, \"'lyin\": False, 'lynton': False, 'australia': False, '2015': False, 'manifesto': False, 'promis': False, 'brexit': False, 'brit': False, 'eu27': False, 'allow': False, 'referendum': False, 'gerrymand': False, 'movement': False, 'extens': False, 'complet': False, 'joke': False, 'court': False, 'fond': False, 'camera\"': False, 'awesom': False, 'human': False, 'goober': False, 'uninform': False, 'forgot': False, 'foster': False, 'child': False, 'brainless': False, 'leaders\"': False, 'crookedhillari': False, 'trump\"': False, 'supposedli': False, 'moment': False, 'audienc': False, 'shitless': False, 'top': False, 'brass': False, 'speak': False, 'sick': False, 'woman': False, 'accus': False, 'find': False, 'far': False, 'pray': False, 'weak': False, 'collin': False, 'flake': False, 'pull': False, 'direct': False, 'panther': False, 'bear': False, 'arm': False, '25': False, 'oct': False, '2005': False, 'brazil': False, '‚Äúno‚Äù': False, 'govern': False, 'implement': False, 'factor': False, '639': False, 'brazilian': False, 'violat': False, 'wzero': False, 'effect': False, 'method': False, 'self': False, 'defens': False, 'sin': False, 'deceiv': False, 'truth': False, 'confess': False, 'faith': False, 'forgiv': False, 'purifi': False, 'unright': False, 'john': False, '15-9\"': False, 'million': False, 'permit': False, 'carri': False, 'conceal': False, 'firearm': False, 'anyon': False, 'els': False, 'idiot': False, '\\U0001f928': False, 'media': False, 'incid': False, 'selv': False, 'freak': False, 'hyster': False, 'dedic': False, 'regul': False, 'stripe': False, 'land': False, 'abyss': False, 'owner': False, 'culo': False, 'scar': False, 'wing': False, 'activist': False, 'liar': False, \"who'\": False, 'evid': False, 'christineblaseyford': False, 'confirmkavanaugh': False, 'weakrino': False, 'republican': False, 'reagan': False, 'sign': False, 'bill': False, 'governor': False, 'ca': False, 'awe': False, 'hogwash': False, 'overst': False, 'obviou': False, '30+': False, 'judge6': False, 'backgroundsnot': False, 'peeplink': False, 'trumpboom': False, 'rape': False, 'assum': False, \"answer'\": False, 'stoog': False, 'ocasio-cortez': False, '40t': False, 'plan': False, \"america'\": False, 'futur': False, 'surrend': False, 'moron': False, 'serious': False, 'health': False, 'treatment': False, 'confisc': False, 'longer': False, 'danger': False, 'other': False, 'wait': False, 'add': False, 'list': False, 'school': False, 'shootings\"': False, '22': False, 'calib': False, 'handgun': False, 'vile': False, 'includ': False, 'fine': False, 'rena': False, 'dean': False, 'shut': False, 'ontario': False, 'loos': False, 'provinc': False, 'anywher': False, 'tune': False, 'mr': False, 'harper': False, 'oop': False, 'scheer': False, 'mistak': False, 'alik': False, 'sexual': False, 'assault': False, 'fellow': False, 'student': False, '1978': False, 'howev': False, 'realiz': False, 'victim': False, 'felt': False, \"i'd\": False, 'coloss': False, 'terrorist': False, 'msm': False, 'imma': False, 'smarter': False, 'gilmer': False, 'r': False, 'lifeline‚ù§': False, 'h': False, 'guess': False, 'note': False, 'climat': False, 'summer-fall-': False, 'winter-spr': False, 'protestor': False, 'epic': False, 'beatdown': False, 'weapon': False, 'rocki': False, 'possibl': False, 'oliv': False, 'armi': False, 'appreci': False, 'live': False, '‚Äúyeah': False, 'sorted‚Äù': False, 'small': False, 'sip': False, 'wine': False, 'unusu': False, 'emili': False, 'jj': False, 'wouldn‚Äôt': False, 'profil': False, 'hole': False, 'larg': False, 'citi': False, 'somewher': False, 'nowher': False, 'pro': False, 'hunt': False, 'rifl': False, 'vantag': False, 'close': False, 'hour': False, 'polic': False, 'enter': False, 'min': False, 'aha': False, 'individu': False, 'belong': False, 'win': False, 'team\"': False, 'constant': False, 'condit': False, 'self-devalu': False, 'strong': False, 'q': False, 'lefthillaryantifa': False, 'regardless': False, \"they'r\": False, 'not\"': False, 'repl': False, 'demonstr': False, 'zone': False, 'stricter': False, 'pussi': False, 'grab': False, 'glove': False, 'handl': False, 'food': False, 'environ': False, 'abl': False, 'planet': False, 'congratul': False, 'put': False, 'mouth': False, 'simpl': False, 'break': False, 'brain': False, 'beat': False, 'facsist': False, 'lost': False, 'caus': False, \"fascist'\": False, 'hide': False, 'demor': False, 'hate': False, 'ideolog': False, 'forc': False, 'others\"\"': False, 'root': False, 'anyth': False, 'hollywood': False, 'islam': False, 'firemorningjo': False, 'cut': False, 'cloth': False, 'misogyni': False, 'bigotri': False, 'display': False, 'truli': False, \"i'v\": False, 'ever': False, 'brigitt': False, 'gabriel': False, 'experienc': False, 'nightmar': False, 'sound': False, 'alarm': False, 'attent': False, 'australian': False, 'homicid': False, 'dixiecrat': False, 'kkk': False, 'ruin': False, 'primarili': False, 'chains\"\"': False, 'sad': False, 'exploit': False, 'reput': False, 'destroy': False, 'place': False, 'start': False, 'ban': False, 'propos': False, 'racial': False, 'tactic': False, \"'to\": False, \"about'\": False, 'introduc': False, 'sweet': False, 'sooo': False, 'nice': False, 'sit': False, 'sofa': False, 'input': False, '20second': False, 'tmart': False, 'law\"': False, 'rudi': False, 'kamalaharri': False, 'emperor': False, 'taxpay': False, '‚Äòunprecedented‚Äô': False, 'protect': False, 'kamala': False, 'harri': False, 'via': False, 'fat': False, 'plenti': False, 'conf': False, 'guiltyinnoc': False, 'bench': False, 'diff': False, 'journalist': False, 'supremacyalt-right': False, 'capabl': False, 'despit': False, 'proof': False, 'hypothesi': False, 'entir': False, 'told': False, 'oath': False, 'subpoena': False, 'testifi': False, 'lifetim': False, 'appoint': False, 'stake': False, 'orrin': False, 'hatch': False, 'invit': False, 'lawsuit': False, 'y‚Äôall': False, 'gem': False, 'phoni': False, 'along': False, 'second': False, 'account': False, 'block': False, 'less': False, \"follower'\": False, 'discrimin': False, 'inde': False, 'slowli': False, 'thingi': False, 'horribl': False, 'english': False, 'languas': False, 'sorri': False, 'yep': False, 'hitler': False, 'hot': False, 'deserv': False, 'notic': False, 'thoroughli': False, 'disingenu': False, 'engag': False, 'forgotten': False, 'eu': False, 'afford': False, 'goe': False, 'model': False, 'ton': False, 'horizon': False, 'scientist': False, 'discov': False, 'dna': False, 'mix': False, 'neanderthalswould': False, 'ted': False, 'spank': False, 'board': False, 'lead': False, 'age': False, 'servic': False, 'aust': False, 'lasa': False, 'medicar': False, 'local': False, 'alliancewhat': False, 'hell': False, 'mirror': False, 'send': False, 'remind': False, 'duti': False, 'hostileenviron': False, 'priceonlov': False, 'skypefamili': False, 'separ': False, 'border': False, 'children': False, 'cage': False, 'evil\"': False, 'president\"': False, '23': False, 'june': False, '2016': False, 'reject': False, 'craven': False, 'worship': False, '1m': False, 'contemptu': False, 'defi': False, '2017': False, 'ge': False, 'undermin': False, 'democraci': False, 'instead': False, 'aint': False, 'gift': False, 'bird': False, 'return': False, 'homeland': False, 'phone': False, 'compos': False, 'text': False, 'brother': False, 'angri': False, 'yell': False, 'ü§¶\\u200d‚ôÄÔ∏è': False, 'melt': False, 'snow': False, 'who‚Äô': False, 'remain': False, 'turn': False, 'circu': False, 'minut': False, 'turmp': False, 'ass': False, \"ball'\": False, 'central': False, 'feder': False, 'whatilearnedtoday': False, 'fast': False}\n"
     ]
    }
   ],
   "source": [
    "f = find_features(stemmed_corpus[0])\n",
    "\n",
    "print(stemmed_corpus[0])\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT DATA SET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "features_set = [(find_features(sentence),label) for (sentence,label) in d]\n",
    "test_features_set = [(find_features(sentence),label) for (sentence,label) in test_d]\n",
    "\n",
    "#print(features_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature set size : (13240, 2)\n",
      "training set size : (9930, 2)\n",
      "test set size : (3310, 2)\n"
     ]
    }
   ],
   "source": [
    "print('feature set size :', np.shape(features_set))\n",
    "\n",
    "training, testing = model_selection.train_test_split(features_set, test_size = 0.25, random_state = seed)\n",
    "\n",
    "print('training set size :',np.shape(training))\n",
    "print('test set size :', np.shape(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE SEVERAL MODELS\n",
    "\n",
    "names = ['K Nearest Neighbors','Decision Tree','Random Forest','Logistic Regression', 'SGD Classifier', 'Naive Bayes', 'SVM Linear']\n",
    "\n",
    "classifiers = [\n",
    "    \n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100), \n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names,classifiers)\n",
    "m = list(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble accuracy : 84.63949843260188\n"
     ]
    }
   ],
   "source": [
    "## TEST ON TESTINT SET\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = m, voting ='hard', n_jobs =-1))\n",
    "nltk_ensemble.train(features_set)\n",
    "acc2 = nltk.classify.accuracy(nltk_ensemble, test_features_set) *100 \n",
    "print('Ensemble accuracy :',acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute predictions for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(319, 2)\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "true_preds = []\n",
    "\n",
    "print(np.shape(test_features_set))\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = m, voting ='hard', n_jobs =-1))\n",
    "nltk_ensemble.train(features_set)\n",
    "\n",
    "error = 0\n",
    "\n",
    "for (features, label) in test_features_set:\n",
    "    pred = nltk_ensemble.classify(features)\n",
    "    predictions.append(pred)\n",
    "    true_preds.append(label)\n",
    "    \n",
    "    if pred != label:\n",
    "        error += 1\n",
    "    \n",
    "#print(len(predictions))\n",
    "#print((len(predictions) - error)/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |   0   1 |\n",
      "--+---------+\n",
      "0 |<226> 16 |\n",
      "1 |  31 <46>|\n",
      "--+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "cm = ConfusionMatrix(true_preds, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0 1 1 0 0 0 0 1 1 0]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(predictions[:10])\n",
    "print(Y_test[:10])\n",
    "\n",
    "print(type(Y_test))\n",
    "print(type(predictions))\n",
    "\n",
    "#list(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute F1 score and Confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    conf_matrix = np.zeros((2,2))\n",
    "    for i in range(len(y_true)):\n",
    "        conf_matrix[y_true[i],y_pred[i]] += 1\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_precision_f1(confusion_matrix):\n",
    "    true_pred = 0\n",
    "    total_pred = 0\n",
    "    for i in range(len(confusion_matrix)):\n",
    "        true_pos = confusion_matrix[i][i]\n",
    "        true_pred += true_pos\n",
    "        total_pred += sum(confusion_matrix[i])\n",
    "        false_neg = 0\n",
    "        false_pos = 0\n",
    "        for j in range(len(confusion_matrix)):\n",
    "            if j!=i:\n",
    "                false_neg += confusion_matrix[i][j]\n",
    "                false_pos += confusion_matrix[j][i]\n",
    "        recall = true_pos / (true_pos + false_neg)\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "        f1 = 2*(precision * recall)/(precision + recall)\n",
    "        print(\"==========================\")\n",
    "        print(\"For class \", i, \" : \")\n",
    "        print()\n",
    "        print(\"Precision : \", precision)\n",
    "        print(\"Recall : \", recall)\n",
    "        print(\"F1 : \", f1)\n",
    "        print(\"==========================\")\n",
    "    print(\"Classification Rate : \", true_pred/total_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[226.  16.]\n",
      " [ 31.  46.]]\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(true_preds,predictions)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "For class  0  : \n",
      "\n",
      "Precision :  0.8793774319066148\n",
      "Recall :  0.9338842975206612\n",
      "F1 :  0.905811623246493\n",
      "==========================\n",
      "==========================\n",
      "For class  1  : \n",
      "\n",
      "Precision :  0.7419354838709677\n",
      "Recall :  0.5974025974025974\n",
      "F1 :  0.6618705035971223\n",
      "==========================\n",
      "Classification Rate :  0.8526645768025078\n"
     ]
    }
   ],
   "source": [
    "compute_recall_precision_f1(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTE RESULTS CODATLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission = pd.read_csv('testset-taska.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size : 860\n"
     ]
    }
   ],
   "source": [
    "submit_corpus = data_submission['tweet'].tolist()\n",
    "\n",
    "print(\"corpus size :\", len(submit_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of sentence before removing stopwords : 27\n",
      "size of corpus : 860\n",
      "size of sentence after removing stopwords 26\n"
     ]
    }
   ],
   "source": [
    "######## PROCESS TEST DATA\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "submit_processed_corpus = []\n",
    "\n",
    "for sentence in submit_corpus :\n",
    "    processed = remove_emoji(sentence)\n",
    "    processed = replace_users(processed)\n",
    "    processed = replace_url(processed)\n",
    "    processed = remove_white(processed) #remove punctuation + white space\n",
    "    processed = remove_char(processed)\n",
    "    processed = lower(processed)\n",
    "    submit_processed_corpus.append(processed)\n",
    "\n",
    "submit_tokenized_corpus = get_tokenized_corpus(submit_processed_corpus) #tokenize corpus\n",
    "\n",
    "print('size of sentence before removing stopwords :',len(submit_tokenized_corpus[0]))\n",
    "\n",
    "submit_filtered_corpus = []\n",
    "for sentence in submit_tokenized_corpus :\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        if word not in stop_words and word!='': #remove stop words and empty words\n",
    "            new_sentence.append(word)       \n",
    "    submit_filtered_corpus.append(new_sentence)\n",
    "    \n",
    "print('size of corpus :', len(submit_filtered_corpus))\n",
    "    \n",
    "print('size of sentence after removing stopwords',len(submit_filtered_corpus[0]))\n",
    "\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "submit_stemmed_corpus = []\n",
    "for sentence in submit_filtered_corpus :\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        new_sentence.append(ps.stem(word)) #stem words      \n",
    "    submit_stemmed_corpus.append(new_sentence)\n",
    "\n",
    "#print('\\ncorpus insight after pre-processing :',test_stemmed_corpus[:20])\n",
    "#print('\\ncorpus before any pre-processing :', test_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(submit_stemmed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_features_set = [find_features(sentence) for sentence in submit_stemmed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "true_preds = []\n",
    "\n",
    "error = 0\n",
    "\n",
    "for feature in submit_features_set:\n",
    "    pred = nltk_ensemble.classify(feature)\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "860"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_target = {0:'NOT', 1:'OFF'}\n",
    "datalength = data_submission.shape[0]\n",
    "for i in range(datalength):\n",
    "    data_submission.at[i, 'target'] = dico_target[data_submission.at[i, 'prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission = data_submission.drop(['prediction'], axis=1)\n",
    "data_submission = data_submission.drop(['tweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission.to_csv('subpart_a_submit.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "#############################################################################################################################BITE##\n",
    "#######################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
