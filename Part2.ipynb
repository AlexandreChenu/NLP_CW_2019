{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to /Users/mobby/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def  clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(df, text_field):\n",
    "    \n",
    "    #processed = sentence.decode('utf-8')\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    df[text_field] = df[text_field].apply(lambda elem: emoji_pattern.sub(r'', elem))  \n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "def replace_users(df, text_field):\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub('(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)','@USER', elem))  \n",
    "    return df\n",
    "\n",
    "def replace_url(df, text_field):\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','website', elem))  \n",
    "    return df\n",
    "\n",
    "def remove_white(df, text_field):\n",
    "    \n",
    "    df[text_field] = df[text_field].apply(lambda elem: elem.replace(r'[^\\w\\d\\s]',' '))\n",
    "    df[text_field] = df[text_field].apply(lambda elem: elem.replace(r'\\s+', ' '))\n",
    "    df[text_field] = df[text_field].apply(lambda elem: elem.replace(r'^\\s+|\\s+?$',''))\n",
    "    df[text_field] = df[text_field].apply(lambda elem: elem.replace('\\n',''))\n",
    "    return df\n",
    "\n",
    "def remove_stop(df, text_field): #remove stop words\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df[text_field] = df[text_field].apply(lambda elem: elem.apply(lambda x: ' '.join(term for term in x.split() is term not in stop_words)))    \n",
    "    return df\n",
    "\n",
    "def remove_stems(sentence):\n",
    "    \n",
    "    ps = nltk.PorterStemmer()\n",
    "    processed = processed.apply(lambda x: ' '.join(ps.stem(term) for term in x.split()))    \n",
    "    return(processed)\n",
    "\n",
    "\n",
    "def lower_df(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "def preprocessing(df, text_field):\n",
    "    df = remove_emoji(df, text_field)\n",
    "    df = replace_users(df, text_field)\n",
    "    df = replace_url(df, text_field)\n",
    "    df = remove_white(df, text_field)\n",
    "    df = lower_df(df, text_field)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('offenseval-training-v1.tsv', sep='\\t')\n",
    "df_test = pd.read_csv('offenseval-trial.txt',sep=\"\\t\", header=0, names = ['tweet','subtask_a','subtask_b','subtask_c'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNT    39\n",
       "TIN    34\n",
       "TTH     4\n",
       "Name: subtask_b, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['subtask_b'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataoff = dataset[dataset.subtask_a=='OFF']\n",
    "dataoff_test = df_test[df_test.subtask_a == 'OFF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mobby/anaconda3/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/mobby/anaconda3/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/mobby/anaconda3/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/mobby/anaconda3/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/mobby/anaconda3/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/mobby/anaconda3/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/mobby/anaconda3/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/mobby/anaconda3/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/mobby/anaconda3/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/mobby/anaconda3/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "dataoff = preprocessing(dataoff, 'tweet')\n",
    "dataoff_test = preprocessing(dataoff_test, 'tweet')\n",
    "dataoff = clean_text(dataoff, 'tweet')\n",
    "dataoff_test = clean_text(dataoff_test, 'tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataoff_test = dataoff_test[dataoff_test.subtask_b != 'TTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalength = dataoff.shape[0]\n",
    "datatest_length = dataoff_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataoff.reset_index(inplace=True)\n",
    "dataoff_test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mobby/anaconda3/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dico_target = {'UNT': 0, 'TIN': 1}\n",
    "dataoff['target'] = dataoff['tweet'].copy()\n",
    "for i in range(datalength):\n",
    "    dataoff.at[i, 'target'] = dico_target[dataoff.at[i, 'subtask_b']]\n",
    "    \n",
    "dataoff_test['target'] = dataoff_test['tweet'].copy()\n",
    "for i in range(datatest_length):\n",
    "    dataoff_test.at[i, 'target'] = dico_target[dataoff_test.at[i, 'subtask_b']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_corpus(corpus):\n",
    "    tokenized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        tokenized_sentence = []\n",
    "        for token in sentence.split(' '): \n",
    "            tokenized_sentence.append(token)\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    " \n",
    "    return tokenized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataoff\n",
    "df_test = dataoff_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = dataset['tweet'].tolist()\n",
    "corpus_test = df_test['tweet'].tolist()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokenized_corpus_train = get_tokenized_corpus(corpus_train)\n",
    "filtered_corpus_train = []\n",
    "\n",
    "for sentence in tokenized_corpus_train :\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        if word not in stop_words and word!='': #remove stop words and empty words\n",
    "            new_sentence.append(word)       \n",
    "    filtered_corpus_train.append(new_sentence)\n",
    "    \n",
    "tokenized_corpus_test = get_tokenized_corpus(corpus_test)\n",
    "filtered_corpus_test = []\n",
    "\n",
    "for sentence in tokenized_corpus_test :\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        if word not in stop_words and word!='': #remove stop words and empty words\n",
    "            new_sentence.append(word)       \n",
    "    filtered_corpus_test.append(new_sentence)\n",
    "    \n",
    "ps = nltk.PorterStemmer()\n",
    "stemmed_corpus_train = []\n",
    "for sentence in filtered_corpus_train :\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        new_sentence.append(ps.stem(word)) #stem words      \n",
    "    stemmed_corpus_train.append(new_sentence)\n",
    "    \n",
    "stemmed_corpus_test = []\n",
    "for sentence in filtered_corpus_test:\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        new_sentence.append(ps.stem(word)) #stem words      \n",
    "    stemmed_corpus_test.append(new_sentence)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(stemmed_corpus_train)):\n",
    "    sentence = ''\n",
    "    sentence_list = stemmed_corpus_train[i]\n",
    "    for word in sentence_list:\n",
    "        sentence += word\n",
    "        sentence += ' '\n",
    "    dataset.at[i, 'tweet'] = sentence\n",
    "    \n",
    "for i in range(len(stemmed_corpus_test)):\n",
    "    sentence = ''\n",
    "    sentence_list = stemmed_corpus_test[i]\n",
    "    for word in sentence_list:\n",
    "        sentence += word\n",
    "        sentence += ' '\n",
    "    df_test.at[i, 'tweet'] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset['tweet']\n",
    "y_train = dataset['target']\n",
    "X_test = df_test['tweet']\n",
    "y_test = df_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4400,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-892995812992>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mros\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mli/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "#Handle imbalanced dataset\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "print(np.shape(X_train))\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train = X_train.reshape(-1,1)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0])\n",
    "X_train = pd.Series(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 11000\n",
    "maxlen = 100\n",
    "list_train = X_train.values\n",
    "list_test = X_test.values\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_precision_f1(confusion_matrix):\n",
    "    true_pred = 0\n",
    "    total_pred = 0\n",
    "    total_f1 = 0\n",
    "    for i in range(len(confusion_matrix)):\n",
    "        true_pos = confusion_matrix[i][i]\n",
    "        true_pred += true_pos\n",
    "        total_pred += sum(confusion_matrix[i])\n",
    "        false_neg = 0\n",
    "        false_pos = 0\n",
    "        for j in range(len(confusion_matrix)):\n",
    "            if j!=i:\n",
    "                false_neg += confusion_matrix[i][j]\n",
    "                false_pos += confusion_matrix[j][i]\n",
    "        recall = true_pos / (true_pos + false_neg)\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "        f1 = 2*(precision * recall)/(precision + recall)\n",
    "        total_f1 += f1\n",
    "        print(\"==========================\")\n",
    "        print(\"For class \", i, \" : \")\n",
    "        print()\n",
    "        print(\"Precision : \", precision)\n",
    "        print(\"Recall : \", recall)\n",
    "        print(\"F1 : \", f1)\n",
    "        print(\"==========================\")\n",
    "    print(\"Classification Rate : \", true_pred/total_pred)\n",
    "    print(\"Macro Average Classification Rate : \", total_f1/len(confusion_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    conf_matrix = np.zeros((2,2))\n",
    "    for i in range(len(y_true)):\n",
    "        conf_matrix[y_true[i],y_pred[i]] += 1\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    adam_optim = Adam(lr=0.00009, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001, amsgrad=False)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam_optim,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "batch_size = 64\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
      "Train on 6976 samples, validate on 776 samples\n",
      "Epoch 1/4\n",
      "6976/6976 [==============================] - 15s 2ms/step - loss: 0.6886 - acc: 0.5496 - val_loss: 0.7638 - val_acc: 0.0000e+00\n",
      "Epoch 2/4\n",
      "6976/6976 [==============================] - 13s 2ms/step - loss: 0.6804 - acc: 0.5556 - val_loss: 0.7810 - val_acc: 0.0000e+00\n",
      "Epoch 3/4\n",
      "6976/6976 [==============================] - 13s 2ms/step - loss: 0.6646 - acc: 0.5632 - val_loss: 0.7711 - val_acc: 0.0644\n",
      "Epoch 4/4\n",
      "6976/6976 [==============================] - 13s 2ms/step - loss: 0.6178 - acc: 0.6892 - val_loss: 0.7058 - val_acc: 0.6198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ca314ceb8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_te)\n",
    "y_pred = pd.Series(y_pred.round().astype(int).reshape(y_pred.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12., 27.],\n",
       "       [ 3., 31.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "For class  0  : \n",
      "\n",
      "Precision :  0.8\n",
      "Recall :  0.3076923076923077\n",
      "F1 :  0.4444444444444444\n",
      "==========================\n",
      "==========================\n",
      "For class  1  : \n",
      "\n",
      "Precision :  0.5344827586206896\n",
      "Recall :  0.9117647058823529\n",
      "F1 :  0.6739130434782609\n",
      "==========================\n",
      "Classification Rate :  0.589041095890411\n",
      "Macro Average Classification Rate :  0.5591787439613527\n"
     ]
    }
   ],
   "source": [
    "compute_recall_precision_f1(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute prediction for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission = pd.read_csv('testset-taskb.tsv', sep='\\t')\n",
    "datalength = data_submission.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission = preprocessing(data_submission, 'tweet')\n",
    "data_submission = clean_text(data_submission, 'tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_submission = data_submission['tweet'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus_submission = get_tokenized_corpus(corpus_submission)\n",
    "filtered_corpus_submission = []\n",
    "\n",
    "for sentence in tokenized_corpus_submission :\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        if word not in stop_words and word!='': #remove stop words and empty words\n",
    "            new_sentence.append(word)       \n",
    "    filtered_corpus_submission.append(new_sentence)\n",
    "    \n",
    "ps = nltk.PorterStemmer()\n",
    "stemmed_corpus_submission = []\n",
    "for sentence in filtered_corpus_submission :\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        new_sentence.append(ps.stem(word)) #stem words      \n",
    "    stemmed_corpus_submission.append(new_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(stemmed_corpus_submission)):\n",
    "    sentence = ''\n",
    "    sentence_list = stemmed_corpus_submission[i]\n",
    "    for word in sentence_list:\n",
    "        sentence += word\n",
    "        sentence += ' '\n",
    "    data_submission.at[i, 'tweet'] = sentence\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_submission['tweet']\n",
    "list_test = X_test.values\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_test)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_te)\n",
    "y_pred = pd.Series(y_pred.round().astype(int).reshape(y_pred.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission['prediction'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission = data_submission.drop(['tweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_target = {0:'UNT', 1:'TIN'}\n",
    "for i in range(datalength):\n",
    "    data_submission.at[i, 'target'] = dico_target[data_submission.at[i, 'prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission = data_submission.drop(['prediction'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission.to_csv('subpart_b.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset.subtask_b == 'TIN']\n",
    "df_test = df_test[df_test.subtask_b == 'TIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[df_test.subtask_c != 'ORG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset['tweet']\n",
    "y_train = dataset['subtask_c']\n",
    "X_test = df_test['tweet']\n",
    "y_test = df_test['subtask_c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train[:3300]\n",
    "y_train_ = y_train[:3300]\n",
    "X_test = X_train[3300:]\n",
    "y_test = y_train[3300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IND    357\n",
       "GRP    163\n",
       "OTH     56\n",
       "Name: subtask_c, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_\n",
    "y_train = y_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clement/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Handle imbalanced dataset\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train = X_train.reshape(-1,1)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = np.max(y_train) + 1\n",
    "y_train = np.eye(n_values)[y_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0])\n",
    "X_train = pd.Series(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 11000\n",
    "maxlen = 100\n",
    "list_train = X_train.values\n",
    "list_test = X_test.values\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "    x = Dense(3, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    adam_optim = Adam(lr=0.00029, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0012, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam_optim,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "batch_size = 128\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5535 samples, validate on 615 samples\n",
      "Epoch 1/6\n",
      "5535/5535 [==============================] - 10s 2ms/step - loss: 1.0894 - acc: 0.4134 - val_loss: 1.3121 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "5535/5535 [==============================] - 8s 1ms/step - loss: 1.0744 - acc: 0.4741 - val_loss: 1.3289 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "5535/5535 [==============================] - 8s 1ms/step - loss: 1.0417 - acc: 0.5534 - val_loss: 1.3089 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "5535/5535 [==============================] - 10s 2ms/step - loss: 0.9102 - acc: 0.5680 - val_loss: 1.0923 - val_acc: 0.0195\n",
      "Epoch 5/6\n",
      "5535/5535 [==============================] - 8s 1ms/step - loss: 0.7403 - acc: 0.6650 - val_loss: 0.9642 - val_acc: 0.3138\n",
      "Epoch 6/6\n",
      "5535/5535 [==============================] - 10s 2ms/step - loss: 0.5773 - acc: 0.8005 - val_loss: 0.6155 - val_acc: 0.8244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ca0078860>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_te)\n",
    "y_pred = [np.argmax(y_pred[i]) for i in range(len(y_pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 80,  56,  27],\n",
       "       [ 70, 259,  28],\n",
       "       [ 23,  22,  11]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "For class  0  : \n",
      "\n",
      "Precision :  0.4624277456647399\n",
      "Recall :  0.49079754601226994\n",
      "F1 :  0.47619047619047616\n",
      "==========================\n",
      "==========================\n",
      "For class  1  : \n",
      "\n",
      "Precision :  0.7685459940652819\n",
      "Recall :  0.7254901960784313\n",
      "F1 :  0.7463976945244958\n",
      "==========================\n",
      "==========================\n",
      "For class  2  : \n",
      "\n",
      "Precision :  0.16666666666666666\n",
      "Recall :  0.19642857142857142\n",
      "F1 :  0.180327868852459\n",
      "==========================\n",
      "Classification Rate :  0.6076388888888888\n",
      "Macro Average Classification Rate :  0.4676386798558103\n"
     ]
    }
   ],
   "source": [
    "compute_recall_precision_f1(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute prediction for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will train on the whole dataset\n",
    "\n",
    "X_train = dataset['tweet']\n",
    "y_train = dataset['subtask_c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, text_field):\n",
    "    df = remove_emoji(df, text_field)\n",
    "    df = replace_users(df, text_field)\n",
    "    df = replace_url(df, text_field)\n",
    "    df = remove_white(df, text_field)\n",
    "    df = lower_df(df, text_field)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission = pd.read_csv('test_set_taskc.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission = pd.read_csv('test_set_taskc.tsv', sep='\\t')\n",
    "datalength = data_submission.shape[0]\n",
    "data_submission = preprocessing(data_submission, 'tweet')\n",
    "data_submission = clean_text(data_submission, 'tweet')\n",
    "corpus_submission = data_submission['tweet'].tolist()\n",
    "\n",
    "tokenized_corpus_submission = get_tokenized_corpus(corpus_submission)\n",
    "filtered_corpus_submission = []\n",
    "\n",
    "for sentence in tokenized_corpus_submission :\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        if word not in stop_words and word!='': #remove stop words and empty words\n",
    "            new_sentence.append(word)       \n",
    "    filtered_corpus_submission.append(new_sentence)\n",
    "    \n",
    "ps = nltk.PorterStemmer()\n",
    "stemmed_corpus_submission = []\n",
    "for sentence in filtered_corpus_submission :\n",
    "    new_sentence = []\n",
    "    for word in sentence: \n",
    "        new_sentence.append(ps.stem(word)) #stem words      \n",
    "    stemmed_corpus_submission.append(new_sentence)\n",
    "    \n",
    "for i in range(len(stemmed_corpus_submission)):\n",
    "    sentence = ''\n",
    "    sentence_list = stemmed_corpus_submission[i]\n",
    "    for word in sentence_list:\n",
    "        sentence += word\n",
    "        sentence += ' '\n",
    "    data_submission.at[i, 'tweet'] = sentence\n",
    "    \n",
    "    \n",
    "X_test = data_submission['tweet']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stemmed_corpus_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clement/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Handle imbalanced dataset\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train = X_train.reshape(-1,1)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "n_values = np.max(y_train) + 1\n",
    "y_train = np.eye(n_values)[y_train]\n",
    "X_train = X_train.reshape(X_train.shape[0])\n",
    "X_train = pd.Series(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 11000\n",
    "maxlen = 100\n",
    "list_train = X_train.values\n",
    "list_test = X_test.values\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.05)(x)\n",
    "    x = Dense(3, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    adam_optim = Adam(lr=0.00029, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0012, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam_optim,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "batch_size = 128\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6498 samples, validate on 723 samples\n",
      "Epoch 1/6\n",
      "6498/6498 [==============================] - 11s 2ms/step - loss: 1.0909 - acc: 0.4135 - val_loss: 1.2823 - val_acc: 0.0000e+00\n",
      "Epoch 2/6\n",
      "6498/6498 [==============================] - 10s 1ms/step - loss: 1.0718 - acc: 0.4903 - val_loss: 1.3214 - val_acc: 0.0000e+00\n",
      "Epoch 3/6\n",
      "6498/6498 [==============================] - 9s 1ms/step - loss: 0.9938 - acc: 0.5365 - val_loss: 1.2459 - val_acc: 0.0000e+00\n",
      "Epoch 4/6\n",
      "6498/6498 [==============================] - 9s 1ms/step - loss: 0.7555 - acc: 0.6964 - val_loss: 0.8083 - val_acc: 0.7068\n",
      "Epoch 5/6\n",
      "6498/6498 [==============================] - 9s 1ms/step - loss: 0.4317 - acc: 0.8524 - val_loss: 0.5067 - val_acc: 0.8216\n",
      "Epoch 6/6\n",
      "6498/6498 [==============================] - 9s 1ms/step - loss: 0.2861 - acc: 0.9046 - val_loss: 0.3043 - val_acc: 0.9281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7c5d747e80>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_te)\n",
    "y_pred = [np.argmax(y_pred[i]) for i in range(len(y_pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission['prediction'] = y_pred\n",
    "data_submission = data_submission.drop(['tweet'], axis=1)\n",
    "dico_target = {0:'GRP', 1:'IND', 2:'OTH'}\n",
    "for i in range(datalength):\n",
    "    data_submission.at[i, 'target'] = dico_target[data_submission.at[i, 'prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_submission = data_submission.drop(['prediction'], axis=1)\n",
    "data_submission.to_csv('subpart_c.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IND    2407\n",
       "GRP    1074\n",
       "OTH     395\n",
       "Name: subtask_c, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['subtask_c'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_submission['id'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15923</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60133</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83681</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65507</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34263</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49139</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>58995</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88490</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>46444</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>60587</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>44546</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>51628</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>40110</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15998</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>96457</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>70841</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46139</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>40386</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>98916</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32190</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>27550</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>24040</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>73516</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>88905</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>42112</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37740</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>72405</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>47445</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27158</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17183</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>39400</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>88745</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>70443</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>90327</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>65545</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>58543</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>79222</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>72401</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>31354</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>29008</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>41590</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>72523</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>14640</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>74909</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>96397</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>34030</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>29113</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>11286</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>41821</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>76379</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>52080</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>51762</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>71592</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>78688</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>76135</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>30778</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>22569</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>48938</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>41438</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>73439</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id target\n",
       "0    15923    GRP\n",
       "1    60133    GRP\n",
       "2    83681    IND\n",
       "3    65507    IND\n",
       "4    34263    IND\n",
       "5    49139    IND\n",
       "6    58995    IND\n",
       "7    88490    GRP\n",
       "8    46444    IND\n",
       "9    60587    IND\n",
       "10   44546    GRP\n",
       "11   51628    IND\n",
       "12   40110    GRP\n",
       "13   15998    IND\n",
       "14   96457    GRP\n",
       "15   70841    IND\n",
       "16   46139    GRP\n",
       "17   40386    GRP\n",
       "18   98916    GRP\n",
       "19   32190    IND\n",
       "20   27550    IND\n",
       "21   24040    GRP\n",
       "22   73516    IND\n",
       "23   88905    GRP\n",
       "24   42112    IND\n",
       "25   37740    IND\n",
       "26   72405    GRP\n",
       "27   47445    GRP\n",
       "28   27158    IND\n",
       "29   17183    IND\n",
       "..     ...    ...\n",
       "183  39400    IND\n",
       "184  88745    IND\n",
       "185  70443    OTH\n",
       "186  90327    IND\n",
       "187  65545    GRP\n",
       "188  58543    GRP\n",
       "189  79222    GRP\n",
       "190  72401    IND\n",
       "191  31354    IND\n",
       "192  29008    IND\n",
       "193  41590    IND\n",
       "194  72523    IND\n",
       "195  14640    GRP\n",
       "196  74909    IND\n",
       "197  96397    GRP\n",
       "198  34030    IND\n",
       "199  29113    IND\n",
       "200  11286    IND\n",
       "201  41821    IND\n",
       "202  76379    GRP\n",
       "203  52080    GRP\n",
       "204  51762    IND\n",
       "205  71592    IND\n",
       "206  78688    GRP\n",
       "207  76135    IND\n",
       "208  30778    IND\n",
       "209  22569    GRP\n",
       "210  48938    IND\n",
       "211  41438    IND\n",
       "212  73439    IND\n",
       "\n",
       "[213 rows x 2 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatest = pd.read_csv('subpart_c.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
